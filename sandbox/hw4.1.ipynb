{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BE/Bi 103, Fall 2016: Homework 4\n",
    "## Due 1pm, Sunday, October 23\n",
    "\n",
    "(c) 2016 Justin Bois. With the exception of the images, this work is licensed under a [Creative Commons Attribution License CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/). All code contained therein is licensed under an [MIT license](https://opensource.org/licenses/MIT).\n",
    "\n",
    "*This homework was generated from an Jupyter notebook.  You can download the notebook [here](hw4.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import pandas as pd\n",
    "#import scipy.stats as st\n",
    "import statsmodels.tools.numdiff as smnd\n",
    "import numpy as np\n",
    "import numba\n",
    "import random\n",
    "import math\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "random.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1: Writing your own MCMC sampler (60 pts + 40 pts extra credit)\n",
    "\n",
    "**a)** Write your own MCMC sampler that employs a Metropolis-Hastings algorithm that uses a Gaussian proposal distribution. Since you are sampling multiple parameters, your proposal distribution will be multi-dimensional. You can use a Gaussian proposal distribution with a diagonal covariance. In other words, you generate a proposal for each variable in the posterior independently.\n",
    "\n",
    "You can organize your code how you like, but here is a suggestion.\n",
    "\n",
    "* Write a function that takes (or rejects) a Metropolis-Hastings step. It should look something like the below (obviously where it does something instead of `pass`ing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def mh_step(x, log_post, log_post_current, sigma, args=()):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_variables,)\n",
    "        The present location of the walker in parameter space.\n",
    "    log_post : function\n",
    "        The function to compute the log posterior. It has call\n",
    "        signature `log_post(x, *args)`.\n",
    "    log_post_current : float\n",
    "        The current value of the log posterior.\n",
    "    sigma : ndarray, shape (n_variables, )\n",
    "        The standard deviations for the proposal distribution.\n",
    "    args : tuple\n",
    "        Additional arguments passed to `log_post()` function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_out : ndarray, shape (n_variables,)\n",
    "        The position of the walker after the Metropolis-Hastings\n",
    "        step. If no step is taken, returns the inputted `x`.\n",
    "    log_post_updated : float\n",
    "        The log posterior after the step.\n",
    "    accepted : bool\n",
    "        True is the proposal step was taken, False otherwise.\n",
    "    \"\"\"\n",
    "    mu, inv_cov = args\n",
    "    \n",
    "    #Sample next point\n",
    "    nextPoint = np.random.multivariate_normal(x, cov)\n",
    "\n",
    "    #Calculate metropolis ratio\n",
    "    logmetropolisRatio = log_post(nextPoint, *args) - log_post_current\n",
    "    \n",
    "    #This is converting the log metropolis ratio to an actual value between 0 and 1 for probability\n",
    "    logMetropolisRatioEx = np.exp(log_post(nextPoint, *args))/ np.exp(log_post_current)\n",
    "    \n",
    "    #Random decimal between 0 and 1 to decide if we should proceed with new point at a certain probability\n",
    "    n = random.uniform(0,1)\n",
    "    \n",
    "    #If metropolis ratio is >= 1 or the random n is less than the probability (meaning we proceed with the new point)\n",
    "    #at a probability of logMetropolisRatioEx\n",
    "    if (logmetropolisRatio >= 1 or n <= logMetropolisRatioEx):\n",
    "        return nextPoint, log_post(np.array(nextPoint), *args), True\n",
    "    else:\n",
    "        return x, log_post_current, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write another function that calls that function over and over again to do the sampling. It should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mh_sample(log_post, x0, sigma, args=(), n_burn=1000, n_steps=5000,\n",
    "              variable_names=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_post : function\n",
    "        The function to compute the log posterior. It has call\n",
    "        signature `log_post(x, *args)`.\n",
    "    x0 : ndarray, shape (n_variables,)\n",
    "        The starting location of a walker in parameter space.\n",
    "    sigma : ndarray, shape (n_variables, )\n",
    "        The standard deviations for the proposal distribution.\n",
    "    args : tuple\n",
    "        Additional arguments passed to `log_post()` function.\n",
    "    n_burn : int, default 1000\n",
    "        Number of burn-in steps.\n",
    "    n_steps : int, default 1000\n",
    "        Number of steps to take after burn-in.\n",
    "    variable_names : list, length n_variables\n",
    "        List of names of variables. If None, then variable names\n",
    "        are sequential integers.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : DataFrame\n",
    "        The first `n_variables` columns contain the samples.\n",
    "        Additionally, column 'lnprob' has the log posterior value\n",
    "        at each sample.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    lnprob = []\n",
    "    finalPoint = x0\n",
    "    new_mu, new_cov = args\n",
    "    logPostCurrent = log_post(x0, *args)\n",
    "    isAccepted = True\n",
    "    \n",
    "    #Burn-in period\n",
    "    \n",
    "    for i in range(n_burn):\n",
    "        finalPoint, logPostCurrent, isAccepted = mh_step(finalPoint, log_post, logPostCurrent, sigma, args)\n",
    "    \n",
    "    #After burn-in, we actually log in sample and log posterior values\n",
    "    for j in range(n_steps):\n",
    "        finalPoint, logPostCurrent, isAccepted = mh_step(finalPoint, log_post, logPostCurrent, sigma, args)\n",
    "        samples.append(finalPoint)\n",
    "        lnprob.append(logPostCurrent)\n",
    "    d = {'Samples': samples, 'Log Posterior Value': lnprob}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** To test your code, we will get samples out of a known distribution. We will use a bivariate Gaussian with a mean of $\\boldsymbol{\\mu} = (10, 20)$ and covariance matrix of \n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\sigma} = \\begin{pmatrix}\n",
    "4 & -2 \\\\\n",
    "-2 & 6\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "I have written the function to be unnormalized and JITted with numba for optimal speed.\n",
    "\n",
    "Do not be confused: In this test function we are sampling $\\mathbf{x}$ out of $P(\\mathbf{x}\\mid \\boldsymbol{\\mu},\\boldsymbol{\\sigma})$. This is not sampling a posterior; it's just a test for your code. You will pass `log_test_distribution` as the `log_post` argument in the above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = np.array([10.0, 20])\n",
    "cov = np.array([[4, -2],[-2, 6]])\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def log_test_distribution(x, mu, inv_cov):\n",
    "    \"\"\"\n",
    "    Unnormalized log posterior of a multivariate Gaussian.\n",
    "    \"\"\"\n",
    "    return -np.dot((x-mu), np.dot(inv_cov, (x-mu))) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log Posterior Value</th>\n",
       "      <th>Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.262771</td>\n",
       "      <td>[6.28619206062, 19.5358582739]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.262771</td>\n",
       "      <td>[6.28619206062, 19.5358582739]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.935100</td>\n",
       "      <td>[7.71754226917, 16.3622134527]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.015843</td>\n",
       "      <td>[11.4389134028, 15.0888445439]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.590648</td>\n",
       "      <td>[13.526044142, 17.6326222631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.221823</td>\n",
       "      <td>[13.0006388587, 17.5181292499]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.065307</td>\n",
       "      <td>[12.806758758, 17.6990131196]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.787241</td>\n",
       "      <td>[12.4664751218, 18.2490388006]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.787241</td>\n",
       "      <td>[12.4664751218, 18.2490388006]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.465652</td>\n",
       "      <td>[11.8367395297, 19.7445832008]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.432682</td>\n",
       "      <td>[9.32284270818, 18.4011468755]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.113331</td>\n",
       "      <td>[9.04957613221, 20.5398116203]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.475200</td>\n",
       "      <td>[11.3624628253, 20.8781333783]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.475200</td>\n",
       "      <td>[11.3624628253, 20.8781333783]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.214584</td>\n",
       "      <td>[12.5179704537, 20.7954309663]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.469013</td>\n",
       "      <td>[11.9052345442, 19.4381972311]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.469013</td>\n",
       "      <td>[11.9052345442, 19.4381972311]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.469013</td>\n",
       "      <td>[11.9052345442, 19.4381972311]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.469013</td>\n",
       "      <td>[11.9052345442, 19.4381972311]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.469013</td>\n",
       "      <td>[11.9052345442, 19.4381972311]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.017940</td>\n",
       "      <td>[9.86230301959, 19.6742563293]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.017940</td>\n",
       "      <td>[9.86230301959, 19.6742563293]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.017940</td>\n",
       "      <td>[9.86230301959, 19.6742563293]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.083208</td>\n",
       "      <td>[9.97906978823, 19.0985824789]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.253406</td>\n",
       "      <td>[9.48494305156, 23.7507303078]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.596492</td>\n",
       "      <td>[9.45758431414, 22.6370386007]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.596492</td>\n",
       "      <td>[9.45758431414, 22.6370386007]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.117190</td>\n",
       "      <td>[9.69056326137, 19.1289441961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.117190</td>\n",
       "      <td>[9.69056326137, 19.1289441961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.117190</td>\n",
       "      <td>[9.69056326137, 19.1289441961]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>-3.937675</td>\n",
       "      <td>[12.9366811591, 23.8792410259]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>-3.937675</td>\n",
       "      <td>[12.9366811591, 23.8792410259]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>-3.937675</td>\n",
       "      <td>[12.9366811591, 23.8792410259]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>-0.867265</td>\n",
       "      <td>[11.3783614756, 21.820360777]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>-1.274615</td>\n",
       "      <td>[11.7700195474, 22.0865110473]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>-0.774660</td>\n",
       "      <td>[12.3008195554, 19.9123140009]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>-0.395824</td>\n",
       "      <td>[11.6952372155, 19.757323812]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>-0.212710</td>\n",
       "      <td>[10.9881892915, 20.4579853229]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>-0.212710</td>\n",
       "      <td>[10.9881892915, 20.4579853229]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>-0.521312</td>\n",
       "      <td>[8.11934674795, 20.0503527962]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>-1.158762</td>\n",
       "      <td>[10.1052657213, 16.5453422255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>-1.158762</td>\n",
       "      <td>[10.1052657213, 16.5453422255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>-0.577537</td>\n",
       "      <td>[8.01932812695, 20.0567730188]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>-0.406348</td>\n",
       "      <td>[10.0874385155, 21.9697170235]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>-0.406348</td>\n",
       "      <td>[10.0874385155, 21.9697170235]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>-0.699643</td>\n",
       "      <td>[11.26920518, 21.5976227458]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>-0.699643</td>\n",
       "      <td>[11.26920518, 21.5976227458]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>-0.699643</td>\n",
       "      <td>[11.26920518, 21.5976227458]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>-0.590772</td>\n",
       "      <td>[9.25718759039, 22.6557023058]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>-0.590772</td>\n",
       "      <td>[9.25718759039, 22.6557023058]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>-0.664878</td>\n",
       "      <td>[12.1112886529, 19.9820708751]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>-0.664878</td>\n",
       "      <td>[12.1112886529, 19.9820708751]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>-0.664878</td>\n",
       "      <td>[12.1112886529, 19.9820708751]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>-0.534156</td>\n",
       "      <td>[11.5972686887, 20.6684965208]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>-0.534156</td>\n",
       "      <td>[11.5972686887, 20.6684965208]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>-1.000282</td>\n",
       "      <td>[9.68761057196, 23.2995742607]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>-0.502768</td>\n",
       "      <td>[8.70517680014, 22.3597107]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-1.891642</td>\n",
       "      <td>[8.08558295435, 24.7433915217]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>-1.891642</td>\n",
       "      <td>[8.08558295435, 24.7433915217]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>-2.262372</td>\n",
       "      <td>[7.24556871001, 25.0021442075]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Log Posterior Value                         Samples\n",
       "0               -2.262771  [6.28619206062, 19.5358582739]\n",
       "1               -2.262771  [6.28619206062, 19.5358582739]\n",
       "2               -2.935100  [7.71754226917, 16.3622134527]\n",
       "3               -2.015843  [11.4389134028, 15.0888445439]\n",
       "4               -1.590648   [13.526044142, 17.6326222631]\n",
       "5               -1.221823  [13.0006388587, 17.5181292499]\n",
       "6               -1.065307   [12.806758758, 17.6990131196]\n",
       "7               -0.787241  [12.4664751218, 18.2490388006]\n",
       "8               -0.787241  [12.4664751218, 18.2490388006]\n",
       "9               -0.465652  [11.8367395297, 19.7445832008]\n",
       "10              -0.432682  [9.32284270818, 18.4011468755]\n",
       "11              -0.113331  [9.04957613221, 20.5398116203]\n",
       "12              -0.475200  [11.3624628253, 20.8781333783]\n",
       "13              -0.475200  [11.3624628253, 20.8781333783]\n",
       "14              -1.214584  [12.5179704537, 20.7954309663]\n",
       "15              -0.469013  [11.9052345442, 19.4381972311]\n",
       "16              -0.469013  [11.9052345442, 19.4381972311]\n",
       "17              -0.469013  [11.9052345442, 19.4381972311]\n",
       "18              -0.469013  [11.9052345442, 19.4381972311]\n",
       "19              -0.469013  [11.9052345442, 19.4381972311]\n",
       "20              -0.017940  [9.86230301959, 19.6742563293]\n",
       "21              -0.017940  [9.86230301959, 19.6742563293]\n",
       "22              -0.017940  [9.86230301959, 19.6742563293]\n",
       "23              -0.083208  [9.97906978823, 19.0985824789]\n",
       "24              -1.253406  [9.48494305156, 23.7507303078]\n",
       "25              -0.596492  [9.45758431414, 22.6370386007]\n",
       "26              -0.596492  [9.45758431414, 22.6370386007]\n",
       "27              -0.117190  [9.69056326137, 19.1289441961]\n",
       "28              -0.117190  [9.69056326137, 19.1289441961]\n",
       "29              -0.117190  [9.69056326137, 19.1289441961]\n",
       "...                   ...                             ...\n",
       "4970            -3.937675  [12.9366811591, 23.8792410259]\n",
       "4971            -3.937675  [12.9366811591, 23.8792410259]\n",
       "4972            -3.937675  [12.9366811591, 23.8792410259]\n",
       "4973            -0.867265   [11.3783614756, 21.820360777]\n",
       "4974            -1.274615  [11.7700195474, 22.0865110473]\n",
       "4975            -0.774660  [12.3008195554, 19.9123140009]\n",
       "4976            -0.395824   [11.6952372155, 19.757323812]\n",
       "4977            -0.212710  [10.9881892915, 20.4579853229]\n",
       "4978            -0.212710  [10.9881892915, 20.4579853229]\n",
       "4979            -0.521312  [8.11934674795, 20.0503527962]\n",
       "4980            -1.158762  [10.1052657213, 16.5453422255]\n",
       "4981            -1.158762  [10.1052657213, 16.5453422255]\n",
       "4982            -0.577537  [8.01932812695, 20.0567730188]\n",
       "4983            -0.406348  [10.0874385155, 21.9697170235]\n",
       "4984            -0.406348  [10.0874385155, 21.9697170235]\n",
       "4985            -0.699643    [11.26920518, 21.5976227458]\n",
       "4986            -0.699643    [11.26920518, 21.5976227458]\n",
       "4987            -0.699643    [11.26920518, 21.5976227458]\n",
       "4988            -0.590772  [9.25718759039, 22.6557023058]\n",
       "4989            -0.590772  [9.25718759039, 22.6557023058]\n",
       "4990            -0.664878  [12.1112886529, 19.9820708751]\n",
       "4991            -0.664878  [12.1112886529, 19.9820708751]\n",
       "4992            -0.664878  [12.1112886529, 19.9820708751]\n",
       "4993            -0.534156  [11.5972686887, 20.6684965208]\n",
       "4994            -0.534156  [11.5972686887, 20.6684965208]\n",
       "4995            -1.000282  [9.68761057196, 23.2995742607]\n",
       "4996            -0.502768     [8.70517680014, 22.3597107]\n",
       "4997            -1.891642  [8.08558295435, 24.7433915217]\n",
       "4998            -1.891642  [8.08558295435, 24.7433915217]\n",
       "4999            -2.262372  [7.24556871001, 25.0021442075]\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out = mh_sample(log_test_distribution, np.array([5, 15]), cov, args=(mu, inv_cov))\n",
    "_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First mean: 9.97091247039\n",
      "Second mean: 20.0392495508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.80708675, -1.7856344 ],\n",
       "       [-1.7856344 ,  5.63800218]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalFirst = 0;\n",
    "totalSecond = 0;\n",
    "count = 0;\n",
    "first = []\n",
    "second = []\n",
    "for i in _out['Samples']:\n",
    "    first.append(i[0])\n",
    "    second.append(i[1])\n",
    "    totalFirst += i[0]\n",
    "    totalSecond += i[1]\n",
    "    count += 1\n",
    "print (\"First mean:\", totalFirst/count)\n",
    "print (\"Second mean:\", totalSecond/count)\n",
    "np.cov(first, second)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the means and covariance from the MCMC closely resembles their true respective values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute the means and covariance (using `np.cov()`) of your samples, you should come close to the inputed means and covariance. You might also want to plot your samples using `corner.corner()` to make sure everything makes sense.\n",
    "\n",
    "**c)** (20 pts extra credit) Add in some logic to your Metropolis-Hastings sampler to enable *tuning*. This is the process of automatically adjusting the $\\sigma$ in the proposal distribution such that the acceptance rate is desirable. The target acceptance rate is about 0.4. The developers of [PyMC3](https://github.com/pymc-devs/pymc3) use the scheme below, which is reasonable.\n",
    "\n",
    "|Acceptance rate|Standard deviation adaptation|\n",
    "|:---:|:-------------------:|\n",
    "| < 0.001        |$\\times$ 0.1|\n",
    "|< 0.05         |$\\times$ 0.5|\n",
    "|< 0.2          |$\\times$ 0.9|\n",
    "|> 0.5          |$\\times$ 1.1|\n",
    "|> 0.75         |$\\times$ 2|\n",
    "|> 0.95         |$\\times$ 10|\n",
    "\n",
    "Be sure to test your code to demonstrate that it works.\n",
    "\n",
    "**d)** (20 pts extra credit) Either adapt the functions you already wrote or write new ones to enable sampling of discrete variables. Again, be sure to test your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 4.1, PART C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated covariance matrix after adjustment to get quality acceptance rate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  7.9197264,  -3.9598632],\n",
       "       [ -3.9598632,  11.8795896]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE BELOW FOR PART C\n",
    "\n",
    "def returnAcceptanceRate(log_post, x0, sigma, args=(), n_burn=1000, n_steps=5000,\n",
    "              variable_names=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_post : function\n",
    "        The function to compute the log posterior. It has call\n",
    "        signature `log_post(x, *args)`.\n",
    "    x0 : ndarray, shape (n_variables,)\n",
    "        The starting location of a walker in parameter space.\n",
    "    sigma : ndarray, shape (n_variables, )\n",
    "        The standard deviations for the proposal distribution.\n",
    "    args : tuple\n",
    "        Additional arguments passed to `log_post()` function.\n",
    "    n_burn : int, default 1000\n",
    "        Number of burn-in steps.\n",
    "    n_steps : int, default 1000\n",
    "        Number of steps to take after burn-in.\n",
    "    variable_names : list, length n_variables\n",
    "        List of names of variables. If None, then variable names\n",
    "        are sequential integers.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : DataFrame\n",
    "        The first `n_variables` columns contain the samples.\n",
    "        Additionally, column 'lnprob' has the log posterior value\n",
    "        at each sample.\n",
    "    \"\"\"\n",
    "    finalPoint = x0\n",
    "    new_mu, new_cov = args\n",
    "    logPostCurrent = log_post(x0, *args)\n",
    "    isAccepted = True\n",
    "    \n",
    "    #This is for fine tuning\n",
    "    totalIsAccepted = 0;\n",
    "    #Burn-in period\n",
    "    \n",
    "    for i in range(n_burn):\n",
    "        finalPoint, logPostCurrent, isAccepted = mh_step(finalPoint, log_post, logPostCurrent, sigma, args)\n",
    "    \n",
    "    #After burn-in, we actually log in sample and log posterior values\n",
    "    for j in range(n_steps):\n",
    "        finalPoint, logPostCurrent, isAccepted = mh_step(finalPoint, log_post, logPostCurrent, sigma, args)\n",
    "        if (isAccepted):\n",
    "            totalIsAccepted += 1;\n",
    "    return (totalIsAccepted/n_steps)\n",
    "\n",
    "\n",
    "# This below updates the cov/sigma until we get a good enough acceptance rate for our data\n",
    "returnAcceptanceRate1 = returnAcceptanceRate(log_test_distribution, np.array([5, 15]), cov , args=(mu, inv_cov))\n",
    "while (returnAcceptanceRate1 < 0.38 or returnAcceptanceRate1 > 0.42):\n",
    "    if returnAcceptanceRate1 < 0.001:\n",
    "        cov = cov * 0.1\n",
    "        returnAcceptanceRate1 = returnAcceptanceRate(log_test_distribution, np.array([5, 15]), cov , args=(mu, inv_cov))\n",
    "    elif returnAcceptanceRate1 < 0.05:\n",
    "        cov = cov * 0.5\n",
    "        returnAcceptanceRate1 = returnAcceptanceRate(log_test_distribution, np.array([5, 15]), cov , args=(mu, inv_cov))\n",
    "    elif returnAcceptanceRate1 < 0.38:\n",
    "        cov = cov * 0.95\n",
    "        returnAcceptanceRate1 = returnAcceptanceRate(log_test_distribution, np.array([5, 15]), cov , args=(mu, inv_cov))\n",
    "    elif returnAcceptanceRate1 > 0.95:\n",
    "        cov = cov * 10\n",
    "        returnAcceptanceRate1 = returnAcceptanceRate(log_test_distribution, np.array([5, 15]), cov , args=(mu, inv_cov))\n",
    "    elif returnAcceptanceRate1 > 0.75:\n",
    "        cov = cov * 2\n",
    "        returnAcceptanceRate1 = returnAcceptanceRate(log_test_distribution, np.array([5, 15]), cov , args=(mu, inv_cov))\n",
    "    elif returnAcceptanceRate1 > 0.42:\n",
    "        cov = cov * 1.05\n",
    "        returnAcceptanceRate1 = returnAcceptanceRate(log_test_distribution, np.array([5, 15]), cov , args=(mu, inv_cov))\n",
    "    else:\n",
    "        break;\n",
    "\n",
    "print (\"The updated covariance matrix after adjustment to get quality acceptance rate\")\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final acceptance rate after updating the cov is : 0.4276\n"
     ]
    }
   ],
   "source": [
    "# Now, with the updated cov, we will run to see the acceptance rate.\n",
    "returnAcceptanceRateFinal = returnAcceptanceRate(log_test_distribution, np.array([5, 15]), cov , args=(mu, inv_cov))\n",
    "print (\"The final acceptance rate after updating the cov is :\", returnAcceptanceRateFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First mean: 9.92998614339\n",
      "Second mean: 20.18159014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4.02814132, -1.95569641],\n",
       "       [-1.95569641,  5.69472192]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out = mh_sample(log_test_distribution, np.array([5, 15]), cov, args=(mu, inv_cov))\n",
    "\n",
    "#Variables for keeping track of totals for both means so we can get average later\n",
    "totalFirst = 0\n",
    "totalSecond = 0\n",
    "count = 0\n",
    "\n",
    "\n",
    "first = []\n",
    "second = []\n",
    "for i in _out['Samples']:\n",
    "    first.append(i[0])\n",
    "    second.append(i[1])\n",
    "    totalFirst += i[0]\n",
    "    totalSecond += i[1]\n",
    "    count += 1\n",
    "print (\"First mean:\", totalFirst/count)\n",
    "print (\"Second mean:\", totalSecond/count)\n",
    "np.cov(first, second)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closely following the scheme provided, we keep updating the cov matrix until we get an acceptance rate that is close to 0.4 as desired. As can be seen above, the final cov/sigma differs from the initial cov sent in, and the acceptance rate with the final cov/sigma is 0.4096, awfully close to 0.4 which is good. Once again, we see that the MCMC calculated means and cov matrix resemble their true respective values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 4.1, PART D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First mean: 10\n",
      "Second mean: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4.15499064, -1.89786765],\n",
       "       [-1.89786765,  5.8781854 ]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE FOR PART D\n",
    "\n",
    "def mh_step_discrete(x, log_post, log_post_current, sigma, args=()):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_variables,)\n",
    "        The present location of the walker in parameter space.\n",
    "    log_post : function\n",
    "        The function to compute the log posterior. It has call\n",
    "        signature `log_post(x, *args)`.\n",
    "    log_post_current : float\n",
    "        The current value of the log posterior.\n",
    "    sigma : ndarray, shape (n_variables, )\n",
    "        The standard deviations for the proposal distribution.\n",
    "    args : tuple\n",
    "        Additional arguments passed to `log_post()` function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_out : ndarray, shape (n_variables,)\n",
    "        The position of the walker after the Metropolis-Hastings\n",
    "        step. If no step is taken, returns the inputted `x`.\n",
    "    log_post_updated : float\n",
    "        The log posterior after the step.\n",
    "    accepted : bool\n",
    "        True is the proposal step was taken, False otherwise.\n",
    "    \"\"\"\n",
    "    mu, inv_cov = args\n",
    "    \n",
    "    #Sample next point\n",
    "    nextPoint = np.random.multivariate_normal(x, cov)\n",
    "\n",
    "    #Calculate metropolis ratio\n",
    "    logmetropolisRatio = log_post(nextPoint, *args) - log_post_current\n",
    "    \n",
    "    #This is converting the log metropolis ratio to an actual value between 0 and 1 for probability\n",
    "    logMetropolisRatioEx = np.exp(log_post(nextPoint, *args))/ np.exp(log_post_current)\n",
    "    \n",
    "    #Random decimal between 0 and 1 to decide if we should proceed with new point at a certain probability\n",
    "    n = random.uniform(0,1)\n",
    "    \n",
    "    #Because points must have discrete values, casting is used\n",
    "    nextPoint = [int(round(nextPoint[0])), int(round(nextPoint[1]))]\n",
    "    x = [int(round(x[0])), int(round(x[1]))]\n",
    "    #If metropolis ratio is >= 1 or the random n is less than the probability (meaning we proceed with the new point)\n",
    "    #at a probability of logMetropolisRatioEx\n",
    "    if (logmetropolisRatio >= 1 or n <= logMetropolisRatioEx):\n",
    "        return nextPoint, log_post(np.array(nextPoint), *args), True\n",
    "    else:\n",
    "        return x, log_post_current, False\n",
    "    \n",
    "def mh_sample_discrete(log_post, x0, sigma, args=(), n_burn=1000, n_steps=5000,\n",
    "              variable_names=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_post : function\n",
    "        The function to compute the log posterior. It has call\n",
    "        signature `log_post(x, *args)`.\n",
    "    x0 : ndarray, shape (n_variables,)\n",
    "        The starting location of a walker in parameter space.\n",
    "    sigma : ndarray, shape (n_variables, )\n",
    "        The standard deviations for the proposal distribution.\n",
    "    args : tuple\n",
    "        Additional arguments passed to `log_post()` function.\n",
    "    n_burn : int, default 1000\n",
    "        Number of burn-in steps.\n",
    "    n_steps : int, default 1000\n",
    "        Number of steps to take after burn-in.\n",
    "    variable_names : list, length n_variables\n",
    "        List of names of variables. If None, then variable names\n",
    "        are sequential integers.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : DataFrame\n",
    "        The first `n_variables` columns contain the samples.\n",
    "        Additionally, column 'lnprob' has the log posterior value\n",
    "        at each sample.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    lnprob = []\n",
    "    finalPoint = x0\n",
    "    new_mu, new_cov = args\n",
    "    logPostCurrent = log_post(x0, *args)\n",
    "    isAccepted = True\n",
    "    \n",
    "    #Burn-in period\n",
    "    \n",
    "    for i in range(n_burn):\n",
    "        finalPoint, logPostCurrent, isAccepted = mh_step_discrete(finalPoint, log_post, logPostCurrent, sigma, args)\n",
    "    \n",
    "    #After burn-in, we actually log in sample and log posterior values\n",
    "    for j in range(n_steps):\n",
    "        finalPoint, logPostCurrent, isAccepted = mh_step_discrete(finalPoint, log_post, logPostCurrent, sigma, args)\n",
    "        samples.append(finalPoint)\n",
    "        lnprob.append(logPostCurrent)\n",
    "    d = {'Samples': samples, 'Log Posterior Value': lnprob}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df\n",
    "\n",
    "_out = mh_sample_discrete(log_test_distribution, np.array([5, 15]), cov, args=(mu, inv_cov))\n",
    "\n",
    "#Variables for keeping track of totals for both means so we can get average later\n",
    "totalFirst = 0\n",
    "totalSecond = 0\n",
    "count = 0\n",
    "\n",
    "first = []\n",
    "second = []\n",
    "for i in _out['Samples']:\n",
    "    first.append(i[0])\n",
    "    second.append(i[1])\n",
    "    totalFirst += i[0]\n",
    "    totalSecond += i[1]\n",
    "    count += 1\n",
    "    \n",
    "# Returning means that are discrete which calls for casting\n",
    "print (\"First mean:\", int(round(totalFirst/count)))\n",
    "print (\"Second mean:\", int(round(totalSecond/count)))\n",
    "np.cov(first, second)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take into account sampling of discrete variables, we merely have to cast the returned location of walker to an integer. When first attempting this remedy, we did a mere integer cast by itself, which essentially would always round down whatever value was getting ready to returned. Due to this, we were getting mean estimates that were always below what was inputted. To better use casting, we did an integer cast AFTER rounding the value to the nearest number. This ultimately outputted the means of 10 and 20 which resembles their true values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2: MCMC with Boolean data (40 pts)\n",
    "\n",
    "In this problem, we will work with data of the True/False type. Lots of data sets in the biological sciences are like this. For example, we might look at a certain mutation in *Drosophila* that affects development and we might check whether or not eggs hatch.\n",
    "\n",
    "The data we will use comes from an experiment we did the last couple years in [Bi 1x](http://bi1x.caltech.edu/) here at Caltech. The experiment was developed by Meaghan Sullivan. We studies a neural circuit in *C. elegans* using optogenetics.\n",
    "\n",
    "A neural circuit is a series of interconnected neurons that create a pathway to transmit a signal from where it is received, to where it causes a behavioral response in an animal.  An example is the neural circuit involved in reversals in *C. elegans*.  This circuit consists of three types of neurons: sensory neurons receive stimuli from the environment, command interneurons integrate information from many sensory neurons and pass a signal to the motor neurons, and motor neurons control worm behavior, such as reversals.\n",
    "\n",
    "There are six neurons acting in a circuit that responds to environmental cues and triggers a reversal, a shown in the figure below (based on [Schultheis et. al. 2011](10.1371/journal.pone.0018766)).  These include four sensory neurons (ALM, AVM, ASH, and PLM).  Each sensory neuron is sensitive to a different type of stimulus.  For example, the sensory neuron we are studying (ASH) is sensitive to chemosensory stimuli such as toxins, while another neuron (PLM) is sensitive to mechanical stimuli (touch) in the posterior part of the worm's body.  The sensory neurons send signals that are integrated by two command interneurons (AVA and AVD).  Each sensory neuron can provide an impulse to the command interneurons at any time.  In order for the command interneuron to fire and activate motor neurons, the sum of the stimuli at any point in time must exceed a certain threshold.  Once the stimuli from one or more sensory neurons has induced an action potential in a command interneuron, that signal is passed to motor neurons which will modulate worm behavior.\n",
    "\n",
    "![Reversal neural network](reversal_neural_network.png)\n",
    "\n",
    "In the experiment, we used optogenetics to dissect the function of individual neurons in this circuit.  We worked with two optogenetic worm strains.  The ASH strain has channelrhodopsin (ChR2, represented by a red barrel in the figure above) expressed only in the ASH sensory neuron.  When we shine blue light on this strain, we should activate the ChR2, which will allow sodium and calcium cations to flow into the neuron, simulating an action potential. We want to quantify how robustly this stimulation will cause the worm to exhibit aversion behavior and reverse.  \n",
    "\n",
    "We also studied an AVA strain that has channelrhodopsin expressed only in the AVA command interneuron.  Our goal is to quantify the effects of stimulating this neuron in terms of reversals compared to the ASH neuron and to wild type.\n",
    "\n",
    "The True/False data here are the whether or not the worms undergo a reversal. Here is what the students observed.\n",
    "\n",
    "|Strain|Year|Trials|Reversals|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|WT|2016|36|6|\n",
    "|ASH|2016|35|12|\n",
    "|AVA|2016|36|30|\n",
    "|WT|2015|35|0|\n",
    "|ASH|2015|35|9|\n",
    "|AVA|2015|36|33|\n",
    "\n",
    "For the purposes of this problem, assume that we can pool the results from the two years to have 6/71 reversals for wild type, 21/70 reversals for ASH, and 63/72 reversals for AVA.\n",
    "\n",
    "Our goal is to estimate $p$, the probability of reversal for each strain. That is to say, we want to compute $P(p\\mid r, n, I)$, where $r$ is the number of reversals in $n$ trials.\n",
    "\n",
    "**a)** Write down an expression for the posterior distribution. That is, write down an expression for the likelihood and prior. You can actually compute the evidence analytically, but that is not necessary for this problem; you can just write down the likelihood and prior.\n",
    "\n",
    "**b)** Use your Metropolis-Hastings sampler to sample this posterior for $p$ for each of the three strains. If you were unable to compute problem 3.1, you may use `emcee` to do the sampling. Plot the results. Do you think there is a difference between the wild type and ASH? How about between ASH and AVA?\n",
    "\n",
    "**c)** The posterior plots from part **(b)** are illuminating, but suppose we want to quantify *the difference* in reversal probability between the two strains, say strain 1 and strain 2.  That is, we want to compute $P(\\delta\\mid D, I)$, where $\\delta \\equiv p_2 - p_1$. Use MCMC to compute this, employing your Metropolis-Hastings solver if you can, using `emcee` otherwise.\n",
    "\n",
    "After you have done that with MCMC, it is useful to look at what a pain it is to attempt it analytically. I am just showing this as a lesson to you. To compute the distribution $P(\\delta\\mid D, I)$, we use the fact that we can easily compute the joint probability distribution because the two strains are independent.  To ease notation, we will note that the prior $P(p_1, p_2\\mid I)$ is only nonzero if $0\\le p_1,p_2 \\le 1$, instead of writing it explicitly.  The posterior is\n",
    "\n",
    "\\begin{align}\n",
    "P(p_1, p_2\\mid D, I) &= \n",
    "\\frac{(n_1+1)!\\,(n_2+1)!}{(n_1-n_{r1})!\\,n_{r1}!\\,(n_2 - n_{r2})!\\,n_{r2}!} \\\\[0.5em]\n",
    "&\\;\\;\\;\\;\\;\\;\\;\\;\\times\\,p_1^{n_{r1}}\\,(1-p_1)^{n_1-n_{r1}}\\,p_2^{n_{r2}}\\,(1-p_2)^{n_2-n_{r2}}.\n",
    "\\end{align}\n",
    "\n",
    "We can define new variables $\\delta = p_2 - p_1$ and $\\gamma = p_1 + p_2$.  Then, we have $p_1 = (\\gamma - \\delta) / 2$ and $p_2 = (\\gamma + \\delta) / 2$. \n",
    "Again, to ease notation, we note that $P(\\gamma, \\delta\\mid D, I)$ is nonzero only when $-1 \\le \\delta \\le 1$ and $|\\delta| \\le \\gamma \\le 2 - |\\delta|$.  By the change of variables formula for probability distributions we have\n",
    "\n",
    "\\begin{align}\n",
    "P(\\gamma, \\delta \\mid D, I) =\n",
    "\\begin{vmatrix}\n",
    "\\mathrm{d}p_1/\\mathrm{d}\\gamma & \\mathrm{d}p_1/\\mathrm{d}\\delta \\\\\n",
    "\\mathrm{d}p_2/\\mathrm{d}\\gamma & \\mathrm{d}p_2/\\mathrm{d}\\delta\n",
    "\\end{vmatrix}\n",
    "P(p_1, p_2 \\mid n_{r1}, n_{r2}, n_1, n_2, I)\n",
    "= \\frac{1}{2}\\,P(p_1, p_2 \\mid D, I).\n",
    "\\end{align}\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "\\begin{align}\n",
    "P(\\gamma, \\delta\\mid D, I) &= \\frac{(n_1+1)!\\,(n_2+1)!}{2(n_1-n_{r1})!\\,(n_2-n_{r2})!\\,n_1!\\,n_2!} \\\\\n",
    "&\\;\\;\\;\\;\\times \n",
    "\\left(\\frac{\\gamma-\\delta}{2}\\right)^{n_{r1}}\\,\\left(1-\\frac{\\gamma-\\delta}{2}\\right)^{n_1-n_{r1}} \\left(\\frac{\\gamma+\\delta}{2}\\right)^{n_{r2}}\\,\\left(1-\\frac{\\gamma+\\delta}{2}\\right)^{n_2-n_{r2}}.\n",
    "\\end{align}\n",
    "\n",
    "Finally, to find $P(\\delta\\mid D, I)$, we marginalize by integrating over all possible values of $\\gamma$.\n",
    "\n",
    "\\begin{align}\n",
    "P(\\delta\\mid D, I) =\n",
    "\\int_{|\\delta|}^{2-|\\delta|}\\mathrm{d}\\gamma\\, P(\\gamma, \\delta\\mid D, I)\n",
    "\\end{align}\n",
    "\n",
    "We can expand each of the multiplied terms in the integrand into a polynomial using the binomial theorem, and can then multiply the polynomials together to get a polynomial expression for the integrand.  This can then be integrated.  There is a technical term describing this process: *a big mess*. Ouch. *This* is a major motivation for using MCMC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-111-2f23cc221c32>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-111-2f23cc221c32>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    from scipy.stats import binom\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@numba.jit(nopython=True)\n",
    "from scipy.stats import binom\n",
    "def log_test_binomial(n, p):\n",
    "    \"\"\"\n",
    "    Unnormalized binomial distribution\n",
    "    \"\"\"\n",
    "    return binom(n , p)\n",
    "log_test_binomial(20, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
