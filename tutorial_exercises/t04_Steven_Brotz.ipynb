{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Last week we found the MAP under the assumption of uniform priors. Under this assumption, maximizing the prior also maximizes the likelihood (because they are directly proportional according to Bayes' Law), so by finding the MAP, we also found the maximum likelihood estimation.\n",
    "\n",
    "\n",
    "2) The minimize method is the better alternative in this case because least squares regression is applicable only when the distribution is Gaussian, which is not the case for the data at hand (the log posterior).\n",
    "\n",
    "\n",
    "3) The burn-in period is important because the algorithm starts with random points from the posterior curve at the beginning, which are probably not well chosen. So if we want a distribution that is better representative of the actual posterior, it is better to allow a long burn-in so that the points are better chosen, which will lead to a more accurate distribution.\n",
    "\n",
    "\n",
    "4) Assume all variables except for $a_3$ to be constants."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
