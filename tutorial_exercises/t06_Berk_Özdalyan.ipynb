{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "\n",
    "1. To compare the two models, we need to find the odds ratio, which is given by the ratio of the priors times the ratio of the evidences for the two models. \n",
    "\n",
    "2. The evidence is the integral of the posterior, which may have a complicated, multimodal shape. If the posterior is sharply peaked, we can approximate the integral as the product of width, height and a normalization factor for the nth dimension.\n",
    "\n",
    "3. Now that the equation is set up, we make an educated guess for the priors - if we don't have any clue as to which one is more likely, we can set them equal to each other.\n",
    "\n",
    "4. Next, we find the ratio of the evidences, either by applying the Laplace approximation for sharp-peaked distributions, or by using PTMCMC. \n",
    "\n",
    "5. We're done! If the odds ratio is less than 1, the model in the denominator is more likely. Else, the model in the numerator is more likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: \n",
    "\n",
    "Optimization / Laplace approximation is better if:\n",
    "\n",
    "a) The number of parameters is not too large. (Optimization is costly for a large number of parameters.)\n",
    "\n",
    "b) The posterior distribution is unimodal and sharply peaked. (Laplace approximation is justified.)\n",
    "\n",
    "c) Computational resources are very limited. (Burn + Walk is expensive or computations are too slow to run MCMC)\n",
    "\n",
    "\n",
    "\n",
    "MCMC is better if:\n",
    "\n",
    "a) The number of parameters is too large. (Optimization is too challenging.)\n",
    "\n",
    "b) The posterior is bumpy, multimodal or too broad. (Laplace approximation doesn't hold.)\n",
    "\n",
    "c) We have fast computers! (Can sample, burn and walk by brute force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "It follows from common sense. If we don't have a baseline to compare the predictive power of a model against, we cannot quantitatively say anything about it. Intuitively, that's the same reason why we have a control group and an experimental group in scientific experiments - we cannot observe deviations from \"normal\", background behavior unless we compare our findings with a benchmark. We can't tell if a student is successful if we don't have any data on the academic record of their peers. A project is a \"success\" only if it beats certain expectations, which form a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
