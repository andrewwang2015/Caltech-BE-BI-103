{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. According to Bayes' Law, posterior distribution is the normalized product of the likelihood and prior. Last week, we found the MAP (maximum a posteriori) estimate for the parameter values. In the case of uniform priors (which we assumed to be the case), then finding the MAP is equivalent to finding the MLE. If the prior is uniform (constant with respect to the variables of interest), then the argument that maximizes the posterior also maximizes the likelihood since MAP = MLE * constant. Hence, when we assumed uniform priors during last week's MAP estimation, we also found the parameters that maximize the likelihood. \n",
    "\n",
    "2. The posterior function that is maximized in the tutorial is not in the appropriate form for least squares regression. It cannot be reformulated such that minimizing the sum of squares of a residual function maximizes the posterior. Earlier, our posterior was a product of many decaying exponentials and minimizing the exponent term maximized the posterior. However, that is not true for the posterior in the tutorial. \n",
    "\n",
    "3. The algorithm starts at random points on the posterior curve and it's highly likely that the points are at suboptimal positions. However, if we let them walk around for a sufficiently long period of time and discard the initial N moves, then the resulting \"starting location\" is a lot more likely to converge to the absolute maximum rather than get stuck at a local maximum. As a result, the posterior that we sample will be more representative of the real posterior. The longer we let the points burn, the more likely that our algorithm converges to the right distribution once we run it. In a way, burning is a tradeoff between speed and accuracy. \n",
    "\n",
    "4. We can treat the variables that we don't care about as constants. We can take a slice in the parameter space such that the shape of the resulting posterior is independent of the variables that we wish to marginalize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
