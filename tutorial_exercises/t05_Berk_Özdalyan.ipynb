{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1:\n",
    "\n",
    "mean/std method: Describes all data as a Gaussian with a mean and a standard deviation. Picks the credible region as $μ±kσ$.\n",
    "\n",
    "    Pros: This method is useful if we want to find a quick answer and use a Gaussian approximation near the peak when finding the MAP for each parameter. It's especially useful if the data are approximately normally distributed.\n",
    "\n",
    "    Cons: If the posterior is bounded (e.g. finite), or asymmetrical, then the mean and standard deviation we obtain aren't particularly useful in visualizing the properties of the posterior.\n",
    "    \n",
    "median and quantile approach: Reports the median and the top and bottom $k^{th}$ quantiles. \n",
    "\n",
    "    Pros: Very easy to interpret. Useful for bounded or asymmetrical data. Doesn't assume anything about the posterior.\n",
    "\n",
    "    Cons: If the data are extremely skewed and polarized in two directions, looking at the median and the error bars doesn't help visualize the posterior in any meaningful way. The \"outliers\" may be closer to the median than some points that aren't identified as such.    \n",
    "\n",
    "mode with HPD: Shows the most probable value and the shortest interval that contains $k$% of the data. \n",
    "\n",
    "    Pros: Gives the most probable value and the region that corresponds to the highest posterior density. Useful for identifying the location of the parameters and estimating them.\n",
    "\n",
    "    Cons: Not easily interpretable for those who haven't seen it before. Multimodal distributions cannot be captured just with this method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2:\n",
    "\n",
    "It's okay to throw away a measurement if it comes from a faulty experimental setup (e.g. the heater was broken, so the experiment wasn't conducted at the right temperature.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3: \n",
    "\n",
    "When we say \"heavy-tail\", we mean that the values that are very far from the mean don't have probabilities that are too low. In the extreme case, the uniform distribution can be considered \"heavy-tail\". Heavy-tailed distributions are useful for dealing with outliers because they can account for them in the model. Also, unlike in narrow distributions where the tails vanish rapidly, in heavy-tailed distributions, outliers don't bring down the mean by a lot. (e.g. see Justin's tutorial for the Gaussian fit vs. the Cauchy fit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4:\n",
    "\n",
    "Optimization with many parameters is difficult and computationally intensive. If we were to summarize the posterior with a MAP/error bar using optimization, it would be easier to use the Cauchy distribution since the good/bad data model would likely have too many parameters to fit and would be too complex to solve in a reasonable amount of time if the sample size were large. The Cauchy distribution, however, has a fixed number of parameters that doesn't scale with the data size. Hence, it's a better option for MAP/error bar estimate using optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
