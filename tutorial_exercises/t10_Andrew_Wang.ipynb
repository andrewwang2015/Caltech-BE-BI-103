{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The correlation coefficient gives a measure of how colocalized two images are. Relative to random chance, we now that geting correlation coefficients close to 1 (positively correlated, colocalized) or close to -1 (negatively correlated, not colocalize) tells us level of colocalization.\n",
    "\n",
    "2. As said in the tutorial, colocalization by eye can be deceiving and often subjective/ qualitative. Even with two images merged, it can be hard to say, without doubt, that there is or there is not colocalization. In contrast, a computer can provide actual numbers that are quantative, thereby \"seeing\" much better than humans can.\n",
    "\n",
    "3. For probability of colocalization, we repeatedly scramble one of the images. Then, we compare the colocalization of the two original images to one of the original image with the scrambled image. We then analyze how many the times the colocalization of the two original images is higher. This is very similar to HackerStats and computing p-values where we repeatedly draw samples to see the fraction of times we get a parameter estimate as extreme as what it is in our original data. Both are frequentist.\n",
    "\n",
    "4. We would need to know to which variable the Manders coefficient corresponds to. We can see that from the tutorial, the Manders coefficient for Aubergine is not equal to the Manders coefficeint for Krimper. Also, we would need to know more details on how they define colocalization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
